{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent semantic analysis\n",
    "Note that so far our assumptions have been limited. We developed a method for representing the text of Carmilla as a set of vectors, which collectively formed a vector-space. Using principal component analysis, we reduced the dimensions of that vector-space down into three. In this truncated representation, we examined the graph and looked for meaning both in the points and in the dimensions. Methods where order in data is found through a mathematical process and then meaning ascribed post-hoc, are described as \"unsupervised learning\".\n",
    "\n",
    "The value in these unsupervised methods comes from this reversal. We are neither proving nor modeling a preconcieved idea, but rather examining order already present in the data. So when the quest is to determine \"what we don't know\", this shift in reference frame is a valuable attribute.\n",
    "\n",
    "Latent semantic analysis (LSA) assumes that the vector-space is separable into two ordered matrices. Originally as a process for noise reduction in signals, the implication to a corpus is simple but profound. Every document consists of words, where those words are taken from a topic, and each document is constructed from topics. Or put another way, the author choose topics to cover in each document, and each word came from those selected topics. In customer support, it may be that a customer makes contact to \"complain\" about their \"billing\", both of which would be how someone in customer support may classify a customer contact.\n",
    "\n",
    "Unlike PCA where the vector-space is centralized and then the covariance calculated before the singular value decomposition calculated, in LSA we just calculate the SVD from the word vectors. This means that the original sense of the dimensions is retained and the words that make up the \"topics\" (one of the decomposed matrices) can be examined directly.\n",
    "\n",
    "Rather than view the LSA output here, let's stay with sci-kit learn and review another unsupervised method, k-means clustering. After, we will switch over to the gensim library to go deeper into LSA and then onto Latent Dirichlet Allocation (LDA). I expect that LSA & LDA will form a significant component of the final \"Arion\" package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful stuff\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    }
   ],
   "source": [
    "with open('carmilla.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "from utils.cheaters import dctConstr\n",
    "\n",
    "dct = dctConstr(stop_words=[\"i\", \"you\", \"a\"], ignore_case=True)\n",
    "dct.constructor(corpus)\n",
    "# dct.trimmer(top=5, bottom=10)\n",
    "# dct.build_tfidf(corpus)\n",
    "\n",
    "def split_by_paragraphs(data:str) -> []:\n",
    "    processed=data.lower()\n",
    "    while '\\n\\n\\n' in processed:\n",
    "        processed=processed.replace('\\n\\n\\n','\\n\\n')\n",
    "    out = processed.split('\\n\\n')\n",
    "    return [o.replace(\"\\n\", \" \") for o in out]\n",
    "\n",
    "pcorp = split_by_paragraphs(corpus)\n",
    "pbow = [dct(para) for para in pcorp]\n",
    "ptfidf = [dct.tfidf(para) for para in pcorp]\n",
    "pvec = [dct.bow_to_vec(p) for p in pbow]\n",
    "\n",
    "idx_to_terms = {i:j for j, i in dct.terms.items()} # flip dictionary for reversal\n",
    "print(len(pvec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = decomposition.TruncatedSVD(n_components=100, n_iter=10, random_state=42)\n",
    "lsa.fit(pvec)\n",
    "X = lsa.transform(pvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have taken the corpus vector-space and calulated the \"directions\" within that space that are common as transformed into relationships of documents-topics and topics-words. When we transform the original corpus, we are representing each document in terms of its distribution along each topic.\n",
    "\n",
    "Note that the SVD, places the orthonormal vectors in order of most explained content to least explained. So by taking the original 3988 word dimensions and only selecting the upper 100, we are reducing the \"noise\" in the signal. In this reduced-space, we can once again compare documents. However, this time synonyms etc. will fall along the same axes, bringing similar documents closer together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "[Data Science Manual](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html) provides a good introduction. There is also a discussion regarding the expectation-maximization function, which is key to solving many ML problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=4, n_init=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=4, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n",
      "['narrative', 'hesselius', 'illuminates', 'mysterious', 'and', 'it', 'learning', 'publish', 'relates', 'forestall', 'condensation', 'lady', 'pr√©cis', 'will', 'elaborate', 'paper', 'in', 'treats', 'papers', 'learned']\n",
      "['narrative', 'to', 'written', 'with', 'after', 'remarkable', 'subject', 'the', 'prologue', 'elaborate', 'upon', 'ms', 'usual', 'arcana', 'interest', 'paper', 'accompanies', 'nothing', 'on', 'but']\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "print(np.shape(order_centroids))\n",
    "\n",
    "cluster1 = order_centroids[0,10:30]\n",
    "print([idx_to_terms[i] for i in cluster1])\n",
    "\n",
    "cluster2 = order_centroids[1,10:30]\n",
    "print([idx_to_terms[i] for i in cluster2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What have these in common?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 04\n",
    "- Printing the terms for cluster 3 will result in an error, fix it\n",
    "- Cluster the LSA representation for 7 centers\n",
    "- Plot the first three dimensions of the LSA for all documents\n",
    "- Color the points according to the clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
